{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitasantos/CineMood---PLN/blob/main/2025_Q3_PLN_PROJETO_PR%C3%81TICO_CINEMOOD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6QILOdpOjwv"
      },
      "source": [
        "# **Processamento de Linguagem Natural [2025-Q3]**\n",
        "Prof. Alexandre Donizeti Alves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m67OOx9MX_3"
      },
      "source": [
        "### **PROJETO CINEMOOD** [LangChain + Grandes Modelos de Linguagem]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7hJlilKM485"
      },
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrante 01:**\n",
        "\n",
        "Gustavo Dias Marsili, RA: 11202130401\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "Gustavo Teodoro Bauke, RA: 11202130481\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "Vitória Cordeiro dos Santos, RA: 11202130706"
      ],
      "metadata": {
        "id": "tnIArN0QY-Ek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GRANDE MODELO DE LINGUAGEM (*Large Language Model - LLM*)**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "VbYD2mw8y4CN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">\n",
        "\n",
        "\n",
        "**LLM**: Gemini 2.5 Flash\n",
        "\n",
        ">\n",
        "\n",
        "**Link para a documentação oficial**:\n",
        "https://ai.google.dev/gemini-api/docs?hl=pt-br\n"
      ],
      "metadata": {
        "id": "a6AkE6iW0c3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **API**\n",
        "---"
      ],
      "metadata": {
        "id": "6yExhaebs-nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API**: TMDb API\n",
        "\n",
        "**Site oficial**: https://www.themoviedb.org/\n",
        "\n",
        "**Link para a documentação oficial**: https://developer.themoviedb.org/docs/getting-started\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DjJM_qhEZRy6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtjgWQRzNphL"
      },
      "source": [
        "### **DESCRIÇÃO**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este projeto aplica técnicas de Processamento de Linguagem Natural (PLN), integrando o framework LangChain, um Grande Modelo de Linguagem (LLM) e dados obtidos de uma API externa. O objetivo é criar um sistema capaz de analisar o relato diário de um usuário, extrair informações relevantes e gerar recomendações personalizadas de filmes com base no estado emocional e no conteúdo do texto.\n",
        "\n",
        "O fluxo completo combina coleta de dados da API, análise semântica via LLM e um pipeline de prompts construído com LangChain."
      ],
      "metadata": {
        "id": "fXTwkiiGs2BV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MOTIVAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "eGy7w9LSlGXX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicações modernas de recomendação precisam cada vez mais entender o contexto emocional, o significado e a intenção dos usuários. Em vez de depender apenas de histórico ou avaliações, este projeto demonstra como utilizar um LLM aliado a técnicas clássicas de PLN para interpretar textos do cotidiano e sugerir conteúdos adequados ao momento emocional do indivíduo.\n",
        "\n",
        "A ideia surgiu da pergunta:\n",
        "\"Como um sistema poderia recomendar filmes levando em conta como o usuário está se sentindo e o que ele viveu no dia?\"\n",
        "\n",
        "Este projeto, portanto, ilustra como unir:\n",
        "\n",
        "* LLM para interpretação profunda do texto;\n",
        "\n",
        "* Técnicas de PLN para extrair informações estruturadas;\n",
        "\n",
        "* Dados reais de uma API de filmes para produzir uma recomendação personalizada."
      ],
      "metadata": {
        "id": "umeyamDElUcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TÉCNICAS DE PLN UTILIZADAS**\n",
        "---"
      ],
      "metadata": {
        "id": "sbvRFrAJmdhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Classificação de Sentimentos\n",
        "\n",
        "##O que é:\n",
        "\n",
        "Técnica que identifica o tom emocional predominante em um texto (positivo, negativo, neutro etc.).\n",
        "\n",
        "##Como utilizamos:\n",
        "\n",
        "Criamos um PromptTemplate que instrui o LLM a classificar o texto do usuário em uma das categorias pré-definidas. Este prompt é processado com o LangChain usando LCEL.\n",
        "\n",
        "##Resultado:\n",
        "\n",
        "O sistema determina se o usuário está em um dia “positivo”, “negativo”, “neutro”, “muito positivo” ou “muito negativo”. Essa informação é utilizada posteriormente para guiar a recomendação de gêneros de filmes."
      ],
      "metadata": {
        "id": "Xhu_wUDbmnz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Sumarização (em estilo “Logline”)\n",
        "\n",
        "##O que é:\n",
        "\n",
        "Condensar um texto longo em uma frase curta, mantendo seu núcleo de significado.\n",
        "\n",
        "##Como utilizamos:\n",
        "\n",
        "Criamos um segundo PromptTemplate, instruindo o LLM a atuar como um roteirista e transformar o relato original em uma logline cinematográfica (até 15 palavras), mantendo um tom dramático ou engraçado.\n",
        "\n",
        "##Este resumo tem dois objetivos:\n",
        "\n",
        "- Destacar o “tema central” do dia do usuário.\n",
        "\n",
        "- Servir como um elemento narrativo para a etapa de recomendação de gêneros."
      ],
      "metadata": {
        "id": "BzkI4Lugnqvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **INTEGRAÇÃO COM API EXTERNA (TMDB)**\n",
        "---"
      ],
      "metadata": {
        "id": "YiiXW4JWokEg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O sistema utiliza a API do The Movie Database (TMDB) para:\n",
        "\n",
        "- Buscar a lista oficial de gêneros de filmes;\n",
        "\n",
        "- Mapear cada gênero ao seu respectivo ID;\n",
        "\n",
        "- Após a análise de sentimentos e resumo, solicitar ao LLM que escolha dois gêneros apropriados;\n",
        "\n",
        "- Fazer uma consulta real à API buscando filmes populares nesses gêneros;\n",
        "\n",
        "- Selecionar o filme mais relevante e apresentá-lo ao usuário.\n",
        "\n",
        "Essa etapa garante que o sistema não apenas gere texto, mas também ofereça uma recomendação concreta baseada em dados reais."
      ],
      "metadata": {
        "id": "9_SVtyNjo0N3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **USO DO LANGCHAIN**\n",
        "---"
      ],
      "metadata": {
        "id": "JwkPL5papM1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O LangChain foi utilizado como camada de orquestração entre prompts, LLM e parsing. As principais funcionalidades empregadas foram:\n",
        "\n",
        "- PromptTemplate → estruturação clara dos prompts;\n",
        "\n",
        "- LCEL (|) → criação de pipelines de processamento;\n",
        "\n",
        "- StrOutputParser → garantir saída textual consistente do LLM;\n",
        "\n",
        "- ChatGoogleGenerativeAI → integração do LangChain com o LLM Gemini."
      ],
      "metadata": {
        "id": "z9ze9fJdpRpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IMPLEMENTAÇÃO**\n",
        "---"
      ],
      "metadata": {
        "id": "nw09lujGvfjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain-google-genai google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arc0nV_Sfbtb",
        "outputId": "2286b4b8-14b4-4873-b1ad-a41b2e62e686",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.0.10)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-3.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Collecting google-ai-generativelanguage<1.0.0,>=0.9.0 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain-google-genai)\n",
            "  Using cached langchain_core-1.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.12.3)\n",
            "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting google-generativeai\n",
            "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
            "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
            "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "  Using cached google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "  Using cached google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "  Using cached google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_google_genai-3.0.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_google_genai-3.0.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Using cached langchain_google_genai-3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.80)\n",
            "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting langchain-google-genai\n",
            "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
            "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.43.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (2.32.4)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.47)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.11.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key = GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "1gYw3UMsaUgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "modelo = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", google_api_key = GEMINI_API_KEY)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-amp4Mr5a58w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "vsTekk1Lhzpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Dl2qQRDEV7Y",
        "outputId": "f2d0eaf2-2ac5-4f30-b2a3-c0ecc861c528"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuração da API e Mapeamento de Gêneros\n",
        "O código executa uma requisição GET para a URL configurada (neste caso, a lista de gêneros da API da TMDB), utilizando os cabeçalhos de autorização fornecidos.\n",
        "\n",
        "A partir da resposta dessa requisição, o script realiza três operações de extração de dados do JSON:\n",
        "\n",
        "\n",
        "\n",
        "1.   Gera uma lista contendo apenas os nomes dos gêneros.\n",
        "2.   Gera uma lista contendo apenas os IDs dos gêneros.\n",
        "2.   Cria um dicionário (mapa) que associa diretamente o nome do gênero ao seu ID (exemplo: {'Ação': 28, 'Comédia': 35, ...}), o que é útil para converter nomes em identificadores posteriormente.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z3EWv1XGC8R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.themoviedb.org/3/genre/movie/list?language=pt\"\n",
        "\n",
        "headers = {\n",
        "    \"accept\": \"application/json\",\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiJhY2E5MTRmNDliZDBlYWVhYzM3NjAzMTU2MDBmZTY1MiIsIm5iZiI6MTc2MzMzNTUxOS4yNDEsInN1YiI6IjY5MWE1ZDVmZDA1ZmFkYzc3ZjgxYmVkMSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.ImdO8p0rUZ_h9aI5K5vy8foPV4xx7Asg7vtAwJitaBk\"\n",
        "}\n",
        "\n",
        "\n",
        "response = requests.get(url, headers=headers)\n",
        "lista_generos = [genero['name'] for genero in response.json()['genres']]\n",
        "lista_id_generos = [genero['id'] for genero in response.json()['genres']]\n",
        "mapa_generos = {\n",
        "    genero['name']: genero['id']\n",
        "    for genero in response.json()['genres']\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "X448DPweExzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descrição do dia\n",
        "\n",
        "O código lê a descrição/relato do dia fornecida pelo usuário via input() no Colab/terminal.\n",
        "A partir desse relato, será recomendado um fime ao usuário"
      ],
      "metadata": {
        "id": "9pTSpl_uEOE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "descricao_do_dia = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI3IkP75-SQS",
        "outputId": "338f2e7d-b563-4c7f-a89e-163cfdd93b04"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acordei meio dia. Tinha prometido que ia arrumar o guarda-roupa e estudar pro curso, mas acabei ficando no TikTok por três horas seguidas. Pedi uma pizza agora à noite. Tô me sentindo meio inútil por não ter feito nada produtivo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline de Análise de Sentimento\n",
        "O código define um PromptTemplate específico para classificação de sentimentos. Nele, são configuradas as input_variables (variáveis a serem substituídas) e o template com as instruções para o LLM, solicitando que ele retorne apenas uma categoria textual (como \"positivo\", \"neutro\", etc.).\n",
        "\n",
        "Em seguida, o script constrói um pipeline utilizando a LCEL (LangChain Expression Language). A sintaxe prompt | modelo | StrOutputParser() estabelece um fluxo contínuo onde:\n",
        "\n",
        "\n",
        "\n",
        "1.   O PromptTemplate é preenchido com os dados.\n",
        "2.  O resultado é processado pelo modelo (ChatGoogleGenerativeAI).\n",
        "1.   A saída final passa pelo StrOutputParser para garantir que o retorno seja um texto limpo.\n",
        "\n",
        "Por fim, o pipeline é invocado com o texto do usuário (descricao_do_dia). O resultado é armazenado na variável sentimento e exibido na tela, com espaços em branco excedentes removidos."
      ],
      "metadata": {
        "id": "FzpxZB9aDrwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"texto\"],\n",
        "    template=(\n",
        "        \"Classifique o sentimento do texto como muito positivo, positivo, neutro, negativo e muito negativo. Retorne apenas o sentimento\\n\\n\"\n",
        "        \"Texto: {texto}\\n\\n\"\n",
        "        \"Categoria:\"\n",
        "    )\n",
        ")\n",
        "\n",
        "chain = prompt | modelo | StrOutputParser()\n",
        "sentimento = chain.invoke({\"texto\": descricao_do_dia})\n",
        "print(f\"Categoria: {sentimento.strip()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Po5pnnfL_D5K",
        "outputId": "d3087fa6-4ae8-4c34-8c3d-04f77f6b9d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categoria: Negativo\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geração de Sinopse Criativa (Logline)\n",
        "Este bloco configura a geração de uma \"logline\" (uma sinopse curta). O código define um PromptTemplate com o objetivo de atuar como um roteirista de cinema, condensando o relato do dia do usuário em uma única frase dramática ou engraçada de até 15 palavras, ignorando detalhes irrelevantes.\n",
        "\n",
        "Em seguida, o script monta o pipeline de resumo utilizando a sintaxe prompt_resumo | modelo | StrOutputParser. Ao invocar esse pipeline, ele aplica a técnica de sumarização via LLM no texto fornecido (descricao_do_dia). Por fim, exibe a sinopse gerada na tela, limpando espaços excedentes."
      ],
      "metadata": {
        "id": "s_NkdvSiEoyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_resumo = PromptTemplate(\n",
        "    input_variables=[\"texto\"],\n",
        "    template=(\n",
        "        \"Atue como um roteirista de cinema.\\n\"\n",
        "        \"Resuma o relato do dia do usuário em uma única frase dramática ou engraçada (máximo 15 palavras), \"\n",
        "        \"como se fosse a 'Logline' (sinopse curta) de um filme sobre a vida dele.\\n\"\n",
        "        \"Ignore detalhes irrelevantes.\\n\\n\"\n",
        "        \"Relato original: {texto}\\n\"\n",
        "        \"Sinopse (Logline):\"\n",
        "    )\n",
        ")\n",
        "\n",
        "chain_resumo = prompt_resumo | modelo | StrOutputParser()\n",
        "sinopse_do_dia = chain_resumo.invoke({\"texto\": descricao_do_dia})\n",
        "print(f\"Sinopse Gerada (Sumarização): {sinopse_do_dia.strip()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxwXaA_hILRR",
        "outputId": "acd99213-a13a-46d8-9e23-e64fb7821791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sinopse Gerada (Sumarização): **Um dia de promessas ambiciosas, devorado sem piedade pela irresistível espiral do TikTok.**\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Motor de Recomendação e Regras de Decisão\n",
        "Este bloco configura o sistema de recomendação. Primeiro, define-se um PromptTemplate onde o LLM atua como um especialista em cinema e psicologia. O prompt recebe o texto original, a sinopse gerada e o sentimento detectado para selecionar 2 gêneros de uma lista pré-definida.\n",
        "\n",
        "O template contém regras explícitas de lógica:\n",
        "\n",
        "\n",
        "1.   Sugere animação ou catarse para usuários tristes/entediados.\n",
        "2.   Sugere Ação/Suspense para usuários com energia.\n",
        "1.   Bloqueia recomendações de Comédia em casos de tensão ou desejo de reflexão.\n",
        "\n",
        "O pipeline é construído via LCEL (prompt | modelo) e invocado passando todas as variáveis de contexto necessárias (texto, sentimento, sinopse e lista_generos). O retorno esperado é apenas os nomes dos gêneros, que são então impressos na tela."
      ],
      "metadata": {
        "id": "Jkw49G0FE4Kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"texto\",\"sentimento\",\"sinopse\",\"lista_generos\"],\n",
        "    template=(\n",
        "        \"Você é um especialista em cinema e psicologia. Analise o texto do usuário: '{texto}'.\\n\"\n",
        "        \"Baseado nesta sinopse: '{sinopse}'\\n\"\n",
        "        \"E neste sentimento detectado: {sentimento}.\\n\\n\"\n",
        "        \"Regra de Recomendação:\\n\"\n",
        "        \"1. Se o usuário estiver entediado ou triste, você pode sugerir algo para animar OU algo catártico/profundo.\\n\"\n",
        "        \"2. Se o usuário estiver com energia, sugira Ação ou Suspense.\\n\"\n",
        "        \"3. NÃO recomende Comédia se o texto indicar tensão, raiva ou desejo de reflexão.\\n\\n\"\n",
        "        \"Escolha 2 gêneros desta lista: {lista_generos}.\\n\"\n",
        "        \"Retorne APENAS os nomes dos gêneros separados por vírgula.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "chain = prompt | modelo\n",
        "resposta = chain.invoke({\"texto\": descricao_do_dia, \"sentimento\":sentimento, \"sinopse\":sinopse_do_dia, \"lista_generos\":lista_generos})\n",
        "print(f\"Gênero: {resposta.content.strip()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMkonVovCAN6",
        "outputId": "f20efe38-0787-47c5-b46a-5da2756db9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gênero: Drama, Aventura\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processamento da Resposta e Busca do Filme\n",
        "O código inicia processando a resposta textual do LLM. Utiliza expressões regulares (regex) para separar os nomes dos gêneros sugeridos e, em seguida, busca os IDs correspondentes no dicionário mapa_generos (criado anteriormente). Esses IDs são formatados em uma única string, separados por vírgula, para serem aceitos pela API.\n",
        "\n",
        "Em seguida, são configurados os parâmetros para a rota de descoberta (/discover/movie) da TMDB. Os filtros aplicados incluem:\n",
        "\n",
        "\n",
        "1.   Gêneros: Apenas os IDs selecionados.\n",
        "2.   Ordenação: Popularidade decrescente.\n",
        "\n",
        "\n",
        "1.   Filtros de Qualidade: Mínimo de 50 votos computados e nota média superior a 6.0.\n",
        "2.   Idioma: Português do Brasil.\n",
        "\n",
        "\n",
        "Por fim, o script envia a requisição GET. Se o retorno for bem-sucedido (status 200), ele extrai o JSON, seleciona o primeiro filme da lista (o mais relevante segundo os filtros) e imprime na tela o Título, a Média de Votos e a Sinopse (Overview). Caso contrário, exibe o código de erro."
      ],
      "metadata": {
        "id": "Ukod_rE6FSfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "lista_id = [mapa_generos.get(genero) for genero in re.split(r',\\s*', resposta.content.strip())]\n",
        "ids_formatados = \",\".join(map(str, lista_id))\n",
        "# Parâmetros para a rota /discover/movie da TMDB.\n",
        "# - with_genres: ids concatenados\n",
        "# - sort_by: ordenação por popularidade decrescente\n",
        "# - vote_count.gte: filtra filmes com pelo menos 50 votos\n",
        "# - language: idioma da resposta\n",
        "# - vote_average.gte: filtra filmes com média mínima de avaliação\n",
        "query_params = {\n",
        "    \"with_genres\": ids_formatados,\n",
        "    \"sort_by\": \"popularity.desc\",\n",
        "    \"vote_count.gte\": 50,\n",
        "    \"language\": \"pt-BR\",\n",
        "    \"vote_average.gte\": 6.0\n",
        "}\n",
        "\n",
        "\n",
        "url_filmes = \"https://api.themoviedb.org/3/discover/movie\"\n",
        "\n",
        "response_filmes = requests.get(url_filmes, headers=headers, params=query_params)\n",
        "\n",
        "if response_filmes.status_code == 200:\n",
        "\n",
        "    dados = response_filmes.json()\n",
        "    melhor_filme = dados['results'][0]\n",
        "\n",
        "    print(\"--- Melhor Filme Encontrado ---\")\n",
        "    print(f\"Título: {melhor_filme.get('title')}\")\n",
        "    print(f\"Média de Votos: {melhor_filme.get('vote_average')}\")\n",
        "    print(f\"Visão Geral: {melhor_filme.get('overview')}\")\n",
        "\n",
        "else:\n",
        "    print(f\"Erro na requisição: {response_filmes.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM1Sgh1bGywD",
        "outputId": "86845458-a7ca-40d0-d51e-d9e4b410c3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Melhor Filme Encontrado ---\n",
            "Título: Interestelar\n",
            "Média de Votos: 8.463\n",
            "Visão Geral: As reservas naturais da Terra estão chegando ao fim e um grupo de astronautas recebe a missão de verificar possíveis planetas para receberem a população mundial, possibilitando a continuação da espécie. Cooper é chamado para liderar o grupo e aceita a missão sabendo que pode nunca mais ver os filhos. Ao lado de Brand, Jenkins e Doyle, ele seguirá em busca de um novo lar.\n"
          ]
        }
      ]
    }
  ]
}